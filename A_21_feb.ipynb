{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac6a25-d499-44db-92dd-67142368ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e9a83-0179-48f1-9558-0e3db1df8001",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Web scraping is the process of collecting structured web data in an automated fashion. It's also known as web data extraction. With the help of web \n",
    "scraping, you can extract data from any website, no matter how large is the data, on your computer. Some of the main use cases of web scraping \n",
    "include price monitoring, price intelligence, news monitoring, lead generation, and market research among many others.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ded52-922c-4b53-8c7f-4450c828abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q2. What are the different methods used for Web Scraping?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96de772-3f4a-4d9f-8715-d24d7babefee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Manual Web Scraping: This involves manually copying and pasting data from websites into a spreadsheet or other tool.\n",
    "Web Scraping using a Programming Language: This involves writing code in a programming language such as Python, Ruby, or Java to automate the process\n",
    "of extracting data from websites.\n",
    "Web Scraping Tools and Frameworks: There are several web scraping tools and frameworks available that can help automate the process of extracting data\n",
    "from websites. Some popular tools include BeautifulSoup, Scrapy, and Selenium.\n",
    "APIs: Some websites provide APIs (Application Programming Interfaces) that allow developers to extract data in a structured format. This is a more \n",
    "reliable and efficient method than web scraping, but it requires access to the API and knowledge of programming.\n",
    "Browser Extensions: Some browser extensions, such as Data Miner and Web Scraper, can be used to extract data from websites. These tools work by \n",
    "automating the process of clicking on links, filling out forms, and extracting data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e489ff-a035-4008-a8ac-d89f7e3a5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q3. What is Beautiful Soup? Why is it used?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0183280-9163-477d-9d1b-fc1260ceefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Beautiful Soup is a popular Python library used for web scraping. It allows you to extract data from HTML and XML documents, making it easier to \n",
    "navigate and parse through the structure of a web page.\n",
    "Here are a few reasons why Beautiful Soup is commonly used for web scraping:\n",
    "Easy to Learn and Use: Beautiful Soup is a beginner-friendly library that is easy to learn and use, even for those with little or no experience \n",
    "with Python.\n",
    "Handles Messy HTML: Websites often have messy HTML with inconsistent tags and formatting. Beautiful Soup can handle these inconsistencies and still \n",
    "extract the data you need.\n",
    "Navigable Parse Trees: Beautiful Soup creates a \\\"parse tree\\\" of a web page that you can navigate using Python code, making it easy to locate and \n",
    "extract the data you need.\n",
    "Supports Multiple Parsers: Beautiful Soup supports multiple parsers, including the built-in HTML parser and third-party parsers such as lxml and \n",
    "html5lib, giving you more options for parsing and extracting data from web pages.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d2cdf-b929-4b0a-86db-1d5fc11197ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q4. Why is flask used in this Web Scraping project?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37ff5c-07dd-4d4d-bb86-f7da920aca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Flask is a lightweight and flexible Python web framework that is often used for developing web applications and APIs.\n",
    "Easy to Set Up: Flask is easy to set up and requires minimal configuration, making it a great choice for a small web scraping project.\n",
    "Minimal Overhead: Flask is lightweight and has minimal overhead, which means it can be used to quickly build a simple web application without adding \n",
    "unnecessary complexity.\n",
    "Routing: Flask has a simple routing system that allows you to map URLs to Python functions, making it easy to create custom endpoints for your web \n",
    "scraping project.\n",
    "Template Rendering: Flask includes a built-in template engine called Jinja2, which makes it easy to generate HTML pages and display data in a \n",
    "user-friendly way.\n",
    "Integration with other libraries: Flask can be easily integrated with other Python libraries and tools, including Beautiful Soup, which is commonly \n",
    "used for web scraping.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9966f59-d235-4246-9fde-7929bbbef941",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q5. Write the names of AWS services used in this project. Also, explain the use of each service.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287873d8-7107-421d-b3ca-b6c97d58a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''1.Code Pipeline\n",
    "Code Repository: The first step is to store your code in a code repository, such as AWS CodeCommit, GitHub, or Bitbucket.\n",
    "Build: The second step is to build your code using a build service such as AWS CodeBuild or Jenkins. This step involves compiling the code\n",
    "running tests, and creating an executable or deployable artifact.\n",
    "Testing: The third step is to run automated tests on the build artifact to ensure that it meets the quality standards and functional requirements.\n",
    "Deployment: The fourth step is to deploy the code to a staging environment, such as an AWS Elastic Beanstalk environment or an EC2 instance.\n",
    "Testing in Staging Environment: The fifth step is to run integration tests and perform manual testing in the staging environment to ensure that the \n",
    "code works as expected.\n",
    "Approval: The sixth step is to get approval from stakeholders to deploy the code to the production environment.\n",
    "Deployment to Production: The final step is to deploy the code to the production environment, such as an AWS Elastic Beanstalk environment or an \n",
    "EC2 instance, after it has been approved.\\n\",\n",
    "\n",
    "2. Beanstack\n",
    "AWS Elastic Beanstalk automatically provisions and manages the underlying infrastructure resources required to run applications, such as compute \n",
    "instances, load balancers, and database resources, making it easier to deploy and scale web applications without having to manage the underlying infrastructure.\\n\",\n",
    "\n",
    "AWS Elastic Beanstalk provides an environment that is preconfigured with common services and platforms, including Apache, Nginx, Tomcat, Node.js, \n",
    "PHP, and Docker. It supports a range of web application frameworks, including Django, Flask, Ruby on Rails, Express.js, and Laravel, among others.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
