{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005863f6-3ef4-4290-846b-601507e2f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16bd4e2-4cd3-4978-885b-7044e5b3148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let A be the event that an employee uses the health insurance plan, and B be the event that an employee is a smoker. We are given:\n",
    "\n",
    "P(A) = 0.7 (probability of using the health insurance plan)\n",
    "P(B|A) = 0.4 (probability of being a smoker given that the employee uses the health insurance plan)\n",
    "\n",
    "We need to find P(B|A), the probability of being a smoker given that the employee uses the health insurance plan. We can use Bayes' theorem to calculate this probability:\n",
    "\n",
    "P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "where P(A|B) is the probability of using the health insurance plan given that the employee is a smoker, and P(B) is the overall probability of being a smoker.\n",
    "\n",
    "We are not given P(B), but we can calculate it using the law of total probability:\n",
    "\n",
    "P(B) = P(A and B) + P(A' and B)\n",
    "\n",
    "where A' is the complement of A (i.e., not using the health insurance plan).\n",
    "\n",
    "We can use the information given to calculate P(A and B):\n",
    "\n",
    "P(A and B) = P(B|A) * P(A) = 0.4 * 0.7 = 0.28\n",
    "\n",
    "We can also calculate P(A' and B):\n",
    "\n",
    "P(A' and B) = P(B|A') * P(A') = P(B and A') / P(A') = (1 - P(A)) * P(B|A') / (1 - P(A)) = 0.6 * 0.2 / 0.3 = 0.4\n",
    "\n",
    "Now we can calculate P(B):\n",
    "\n",
    "P(B) = P(A and B) + P(A' and B) = 0.28 + 0.4 = 0.68\n",
    "\n",
    "Finally, we can use Bayes' theorem to calculate P(B|A):\n",
    "\n",
    "P(B|A) = P(A|B) * P(B) / P(A) = P(B and A) / P(A) = 0.28 / 0.7 = 0.4\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.4, or 40%.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae2f96-9004-4683-8984-b91379698531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553d7a8-8b78-4443-af11-9ddbc0d8c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In Bernoulli Naive Bayes, the input features are binary variables (i.e., they can only take on values of 0 or 1), and the algorithm models the presence or absence of a feature in a document. For example, in document classification, a feature could represent the presence or absence of a particular word in the document. The algorithm assumes that the features are independent of each other and calculates the probability of a document belonging to a particular class based on the presence or absence of its features.\n",
    "\n",
    "In Multinomial Naive Bayes, the input features are counts of occurrences of words or tokens in a document. For example, if we are classifying text documents, the features could be the frequency of words in a document. The algorithm calculates the probability of a document belonging to a particular class based on the frequency of occurrence of its features. This algorithm assumes that the features are multinomially distributed, which means that they are drawn from a fixed set of possible values with a fixed probability distribution.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ec121-e527-434d-ae0f-6d237d11cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q3. How does Bernoulli Naive Bayes handle missing values?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa0de5-4be1-4a4e-bc90-7e6478f6cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In Bernoulli Naive Bayes, missing values are usually handled by simply ignoring the corresponding feature when computing the class probabilities. This assumes that the missing values are missing at random and that the absence of a feature is not informative. Ignoring the missing values in this way effectively treats them as if they had a value of zero.\n",
    "\n",
    "If the proportion of missing values in a feature is very high, then that feature may not be informative for classification and may be removed from the analysis entirely. Alternatively, some imputation techniques can be used to estimate the missing values based on the observed values. However, these imputation techniques are not commonly used in Bernoulli Naive Bayes since it assumes that features are binary, and the imputed values may not be binary. Additionally, imputation may introduce bias into the analysis if the missingness is not completely at random.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f9300-fddf-44d3-9606-cfd091c69ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q4. Can Gaussian Naive Bayes be used for multi-class classification?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1993178f-22a4-4275-8d07-61a08940c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Yes, Gaussian Naive Bayes can be used for multi-class classification. In Gaussian Naive Bayes, the algorithm assumes that the features are normally distributed within each class. The class probability is estimated by calculating the product of the likelihood of the features given the class and the prior probability of the class. The class with the highest probability is chosen as the predicted class for a given observation.\n",
    "\n",
    "For multi-class classification, Gaussian Naive Bayes can be extended using the \"one-vs-all\" approach or the \"one-vs-one\" approach. In the \"one-vs-all\" approach, a separate binary classifier is trained for each class, with one class treated as positive and the rest as negative. In the \"one-vs-one\" approach, a separate binary classifier is trained for each pair of classes, with one class from each pair treated as positive and the rest as negative. The class with the most votes across all binary classifiers is chosen as the predicted class for a given observation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d67e831d-1841-481d-bbc9-54d95931d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "spambase_df = pd.read_csv(url, header=None)\n",
    "\n",
    "# Set the column names\n",
    "feature_names = ['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d',\n",
    "                 'word_freq_our', 'word_freq_over', 'word_freq_remove', 'word_freq_internet',\n",
    "                 'word_freq_order', 'word_freq_mail', 'word_freq_receive', 'word_freq_will',\n",
    "                 'word_freq_people', 'word_freq_report', 'word_freq_addresses', 'word_freq_free',\n",
    "                 'word_freq_business', 'word_freq_email', 'word_freq_you', 'word_freq_credit',\n",
    "                 'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money',\n",
    "                 'word_freq_hp', 'word_freq_hpl', 'word_freq_george', 'word_freq_650',\n",
    "                 'word_freq_lab', 'word_freq_labs', 'word_freq_telnet', 'word_freq_857',\n",
    "                 'word_freq_data', 'word_freq_415', 'word_freq_85', 'word_freq_technology',\n",
    "                 'word_freq_1999', 'word_freq_parts', 'word_freq_pm', 'word_freq_direct',\n",
    "                 'word_freq_cs', 'word_freq_meeting', 'word_freq_original', 'word_freq_project',\n",
    "                 'word_freq_re', 'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
    "                 'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!', 'char_freq_$',\n",
    "                 'char_freq_#', 'capital_run_length_average', 'capital_run_length_longest',\n",
    "                 'capital_run_length_total', 'is_spam']\n",
    "spambase_df.columns = feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53dfedf-eee9-46b5-bc21-af4f77991862",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spambase_df.drop('is_spam', axis=1)\n",
    "y = spambase_df['is_spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a08b49-2981-4ac0-b7ea-5e193aeaebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a BernoulliNB instance\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit the model on the training data\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to make predictions on the test data\n",
    "y_pred = bnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0be856d-2224-4b74-a224-57f03e5e5201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8790731354091238\n",
      "Precision: 0.8882575757575758\n",
      "Recall: 0.8128249566724437\n",
      "F1 score: 0.848868778280543\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the performance metrics\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18afc3e3-606f-456f-bd29-0d86310a063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Based on the results obtained from the implementation, the Multinomial Naive Bayes (MNB) variant performed the best among the three variants (Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes). MNB achieved an accuracy of around 89%, while the other two variants achieved an accuracy of around 84%.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
