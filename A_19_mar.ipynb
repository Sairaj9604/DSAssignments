{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0ee38-1d2e-4eb8-8072-cb08e9bdd11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9327f5-94bd-4a5c-b1f1-23af53819220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Missing values in a dataset are the values that are not present for some of the observations or samples. Missing values can occur due to various reasons such as errors in data collection, data corruption, or simply because some of the data was not collected or recorded. Handling missing values is crucial because they can lead to biased and inaccurate results, which can affect the overall analysis.\n",
    "\n",
    "Some of the machine learning algorithms that can handle missing values are:\n",
    "\n",
    "Decision Trees: Decision Trees can handle missing values as they partition the data into smaller subsets based on the feature values, and if a value is missing, the algorithm can still make a decision based on the available data.\n",
    "\n",
    "Random Forest: Random Forest is an ensemble of decision trees, and it can handle missing values by imputing the missing values and then making a decision.\n",
    "\n",
    "K-Nearest Neighbors (KNN): KNN is a non-parametric algorithm that can handle missing values by considering the k-nearest neighbors and then imputing the missing value based on the available data.\n",
    "\n",
    "Naive Bayes: Naive Bayes is a probabilistic algorithm that can handle missing values by ignoring the missing values and making a decision based on the available data.\n",
    "\n",
    "Support Vector Machines (SVM): SVM can handle missing values by imputing the missing values or by using a kernel function that ignores the missing values.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f3b988-cf96-4fb4-a2f8-7c8c08499e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q2: List down techniques used to handle missing data. Give an example of each with python code.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ffbbe-1224-43ae-a037-82170952a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deletion:\n",
    "\n",
    "Listwise deletion: It removes all the rows that have missing values in any column.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe356a-ed99-4eac-b2ee-b8051c6d4ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imputation:\n",
    "\n",
    "Mean, Median or Mode imputation: It replaces the missing values with the mean, median, or mode of the non-missing values in the same column.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ea197-76f9-4bbe-af8f-c8dd3b31f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prediction:\n",
    "\n",
    "Machine learning models: It uses machine learning models to predict the missing values based on the available data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e4f8b-5ba6-4071-b31e-d880cbafae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8abb2-3126-42fb-a15b-87e7966ab1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imbalanced data refers to a situation where the distribution of target classes in a dataset is not equal. In other words, one class has significantly more or fewer samples than the others. For instance, in a binary classification problem, if one class has 90% of the samples, and the other has 10%, then the data is imbalanced.\n",
    "\n",
    "If imbalanced data is not handled correctly, it can lead to biased models that perform poorly in predicting the minority class. For example, if the model is not exposed to enough samples of the minority class during training, it may not learn to recognize it accurately. This can result in higher false negatives and lower recall for the minority class, making the model unreliable.\n",
    "\n",
    "Some of the consequences of not handling imbalanced data are:\n",
    "\n",
    "Poor model performance on the minority class.\n",
    "Biased model predictions.\n",
    "Misleading evaluation metrics such as accuracy, precision, and recall.\n",
    "To handle imbalanced data, we can use techniques such as:\n",
    "\n",
    "Upsampling the minority class: We can increase the number of samples in the minority class by duplicating existing samples or generating synthetic samples using techniques like SMOTE.\n",
    "Downsampling the majority class: We can decrease the number of samples in the majority class by randomly removing samples from it.\n",
    "Class weighting: We can assign higher weightage to the minority class during training to balance out the data.\n",
    "Ensemble techniques: We can use ensemble techniques like bagging and boosting to create models that are less prone to bias.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ea324-b073-4a84-8da7-0edb8266086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce2e0f-bf21-44ae-89ed-ca72f3938b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Up-sampling and down-sampling are techniques used to handle imbalanced datasets. Imbalanced datasets refer to datasets where one class has significantly fewer samples than the other class. Up-sampling refers to the process of increasing the number of samples in the minority class, while down-sampling refers to the process of decreasing the number of samples in the majority class.\n",
    "\n",
    "For example, consider a dataset of credit card transactions where fraudulent transactions account for only 1% of the dataset. In this case, the dataset is imbalanced as the majority class (non-fraudulent transactions) has much more data than the minority class (fraudulent transactions).\n",
    "\n",
    "To handle this imbalanced dataset, we can use up-sampling or down-sampling techniques. In up-sampling, we can create more samples of the minority class by duplicating existing samples or creating new synthetic samples. In down-sampling, we can randomly remove some samples from the majority class.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa60a8a-35d0-454c-b3ee-6f44011b6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q5: What is data Augmentation? Explain SMOTE.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b3b8b-ef1c-4b21-8015-2f2c335bda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data augmentation is a technique used to artificially increase the size of a dataset by creating new variations of the existing data. It helps to improve the performance of machine learning models by introducing more variety into the data.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used for imbalanced datasets. It generates synthetic samples for the minority class by selecting two or more similar samples and interpolating between them to create new ones. This helps to balance the dataset and prevent the model from being biased towards the majority class.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca1ca2d-036f-4910-8126-f280f8232f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.9.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.23.5)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.10.1 imblearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b9b646-5826-4454-ac22-c4b6a774a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# generate an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3,\n",
    "                           n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=10)\n",
    "\n",
    "# apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31049b95-2940-4567-96f2-7c1c871ddaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q6: What are outliers in a dataset? Why is it essential to handle outliers?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5d57f-5047-49b1-8144-ce05d6d22439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Outliers are data points that significantly differ from other observations in the dataset. These observations may be due to measurement or recording errors, or they may be valid but rare data points. Handling outliers is important because they can skew the analysis and results of the model, leading to incorrect predictions.\n",
    "\n",
    "Outliers can impact the statistical analysis, as they can change the mean and standard deviation of the dataset. Additionally, they can influence the performance of machine learning models, as they can increase the variance of the model, leading to overfitting. Outliers can also impact the accuracy of the model, as they may be treated as significant data points, leading to incorrect predictions.\n",
    "\n",
    "Therefore, it is important to identify and handle outliers appropriately. This can involve removing them from the dataset or transforming them into more appropriate values. There are several methods to handle outliers, such as:\n",
    "\n",
    "Z-score method: This method involves calculating the z-score of each data point and removing those that fall beyond a certain threshold.\n",
    "\n",
    "Interquartile range (IQR) method: This method involves calculating the IQR of the dataset and identifying outliers as those that fall outside a certain range of the IQR.\n",
    "\n",
    "Winsorization: This method involves replacing the outliers with the nearest data points within a certain range.\n",
    "\n",
    "Transforming the data: This involves transforming the data into a different scale or distribution, which can reduce the impact of outliers on the analysis.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bc840-7020-494f-9e31-7ce71ceb6e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f53fa6-00ae-4263-8d03-ba2a64d940e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deletion: This technique involves removing the rows or columns that contain missing data. This can be done in two ways:\n",
    "\n",
    "a. Listwise deletion: In this method, any row with missing values is removed from the dataset. This method is also known as complete case analysis.\n",
    "\n",
    "b. Pairwise deletion: In this method, only the rows with missing values are removed. The columns with missing values are still used in the analysis.\n",
    "\n",
    "Imputation: This technique involves filling in the missing values with estimated values. Some common methods for imputation are:\n",
    "\n",
    "a. Mean, Median or Mode imputation: In this method, the missing values are replaced with the mean, median or mode of the non-missing values of that variable.\n",
    "\n",
    "b. Regression imputation: In this method, a regression model is used to predict the missing values based on other variables in the dataset.\n",
    "\n",
    "c. K-nearest neighbor imputation: In this method, missing values are replaced with the average value of the K-nearest neighbors.\n",
    "\n",
    "d. Multiple imputation: In this method, the missing values are imputed multiple times to create multiple complete datasets. The results from these datasets are then combined to get a final result.\n",
    "\n",
    "Predictive models: In this method, predictive models such as Random Forest or XGBoost are used to predict the missing values based on other variables in the dataset.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93d868-7a1e-458c-9180-58695d0b1772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fbf0c1-dc14-46f7-890f-eaf1452ca018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Descriptive statistics: One of the simplest ways to determine if there is a pattern to the missing data is to look at the descriptive statistics of the dataset. If the missing values are distributed randomly, then the descriptive statistics will not be significantly different from those of the complete dataset.\n",
    "\n",
    "Correlation analysis: Correlation analysis can be used to determine if there is a pattern to the missing data. If the missing values are correlated with other variables in the dataset, then there is likely a pattern to the missing data.\n",
    "\n",
    "Data visualization: Data visualization techniques such as scatter plots, box plots, and histograms can be used to identify patterns in the missing data.\n",
    "\n",
    "Imputation methods: Imputation methods can be used to fill in missing values based on patterns in the data. If the imputed values match the pattern of the existing data, then the missing data is likely missing at random.\n",
    "\n",
    "Statistical tests: Statistical tests such as the Little's test or the Missing Completely at Random (MCAR) test can be used to determine if the missing data is missing at random or not.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11426bbe-2193-49c0-814b-644da8d395e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98b26c-f1ad-4480-82ac-0b9731308f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Confusion Matrix: A confusion matrix is a table that shows the number of true positives, true negatives, false positives, and false negatives. This can be used to calculate metrics such as precision, recall, and F1-score, which can help evaluate the model's performance.\n",
    "\n",
    "ROC Curve: A receiver operating characteristic (ROC) curve is a plot of the true positive rate (sensitivity) against the false positive rate (1 - specificity) for different classification thresholds. This can help visualize the trade-off between sensitivity and specificity and can be used to select an appropriate threshold for the model.\n",
    "\n",
    "Precision-Recall Curve: A precision-recall curve is a plot of precision against recall for different classification thresholds. This can help evaluate the model's performance when the positive class is rare.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31e68e-06bb-4132-8b72-62a2740d9ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6418d19-9861-4ecf-8b67-bfbb4aca051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Random Under-Sampling: It involves randomly selecting samples from the majority class to match the size of the minority class.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3d0f3-5e27-4cb5-9ca1-63107a31558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0f608-c65e-4fe1-a9aa-14263300e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Random Oversampling: Randomly replicate samples from the minority class to balance the dataset. This method can be effective, but there is a risk of overfitting since the same data points are being used multiple times.\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): This method creates synthetic samples by generating new samples based on the minority class's nearest neighbors. SMOTE can help to address the overfitting risk associated with random oversampling.\n",
    "ADASYN (Adaptive Synthetic Sampling): Similar to SMOTE, ADASYN creates synthetic samples, but it focuses on generating samples in regions of the feature space where the density of the minority class is lower.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
