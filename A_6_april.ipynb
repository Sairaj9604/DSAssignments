{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdce518-5fd0-4bc5-a137-cfab97d77dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q1. What is the mathematical formula for a linear SVM?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884d4da-7809-4996-9f42-cc0f88ab8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A linear support vector machine (SVM) is a type of binary classification algorithm that tries to find a linear decision boundary between two classes. The mathematical formula for a linear SVM is:\n",
    "\n",
    "w^T x + b = 0\n",
    "\n",
    "where w is the weight vector perpendicular to the hyperplane, x is the feature vector for a data point, b is the bias or intercept term, and the superscript T denotes the transpose operation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27889e3c-978d-4d27-94b8-6966c31b6e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q2. What is the objective function of a linear SVM?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957effc-50d6-40cf-8ba5-45d4761802ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The objective function of a linear support vector machine (SVM) is to find the optimal hyperplane that maximizes the margin between the two classes. The margin is defined as the distance between the hyperplane and the closest data point from each class.\n",
    "\n",
    "The optimization problem can be formulated as a constrained optimization problem:\n",
    "\n",
    "minimize: (1/2) ||w||^2\n",
    "\n",
    "subject to: y_i (w^T x_i + b) >= 1 for all i = 1, ..., n\n",
    "\n",
    "where ||w||^2 is the L2 norm of the weight vector w, y_i is the label (+1 or -1) for the i-th data point, x_i is the feature vector for the i-th data point, and b is the bias term. The constraint ensures that all data points are correctly classified with a margin of at least 1.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98d3c3-8b7f-4575-b899-4b76fb5278da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q3. What is the kernel trick in SVM?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f475aa7a-f69a-4ec2-87a0-00e1569f3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The kernel trick is a technique used in support vector machine (SVM) algorithms to transform data from a low-dimensional space to a high-dimensional space in order to find a better decision boundary. It is based on the idea that it may be easier to find a linear decision boundary in a higher-dimensional space than in the original low-dimensional space.\n",
    "\n",
    "The kernel trick avoids the explicit computation of the feature vectors in the high-dimensional space by using a kernel function that computes the dot product of two feature vectors in the high-dimensional space, without actually computing the feature vectors themselves. This makes the computation more efficient, as the kernel function can be evaluated using the original low-dimensional feature vectors.\n",
    "\n",
    "The most commonly used kernel functions are the linear kernel, polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel. The choice of kernel function depends on the nature of the data and the problem at hand.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab85c1-04b8-4037-866d-b7262ff2b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q4. What is the role of support vectors in SVM Explain with example\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4459ba1-1747-47f7-b749-77039d84fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Support vectors are the data points that lie closest to the decision boundary in a support vector machine (SVM) algorithm. They play a critical role in determining the location and orientation of the decision boundary, and in the generalization performance of the model.\n",
    "\n",
    "In a linear SVM, the decision boundary is a hyperplane that separates the two classes of data points. The support vectors are the data points that lie closest to the hyperplane, and are the ones that define the margin, which is the distance between the hyperplane and the closest data points from each class. These support vectors are the most informative data points, and they determine the location and orientation of the decision boundary.\n",
    "\n",
    "In a non-linear SVM, the decision boundary is a curved surface in a high-dimensional feature space. The support vectors in this case are the data points that lie closest to the decision boundary in the high-dimensional feature space. These support vectors are the most informative data points, and they determine the shape and complexity of the decision boundary.\n",
    "\n",
    "For example, consider a binary classification problem where the goal is to classify a set of data points into two classes: blue and red. The data points are two-dimensional, with x and y coordinates. A linear SVM could be used to find a decision boundary that separates the two classes. The decision boundary would be a straight line in this case.\n",
    "\n",
    "The support vectors are the data points that lie closest to the decision boundary. For example, suppose that there are three blue data points and three red data points, and that the decision boundary is the line y = x. The three blue data points that lie closest to the line would be the support vectors for the blue class, and the three red data points that lie closest to the line would be the support vectors for the red class. These support vectors would be used to define the margin, which is the distance between the line and the closest data points from each class.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062ea65-393a-4d38-a46c-1eab7e07ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ae559-fc71-4c25-ba43-4e4567573d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hyperplane:\n",
    "In a linear SVM, the decision boundary is a hyperplane that separates the two classes of data points. The hyperplane can be represented by the equation:\n",
    "\n",
    "w^T x + b = 0\n",
    "\n",
    "where w is a vector that defines the orientation of the hyperplane, x is the input vector, b is a bias term, and the superscript T denotes the transpose of the vector.\n",
    "\n",
    "For example, consider a binary classification problem where the goal is to classify a set of data points into two classes: blue and red. The data points are two-dimensional, with x and y coordinates. A linear SVM could be used to find a decision boundary that separates the two classes. The decision boundary would be a straight line in this case.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157d3cf-0b0f-41bd-a60e-d17102c425d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Marginal plane:\n",
    "The margin is the distance between the hyperplane and the closest data points from each class. The marginal plane is a plane that is parallel to the hyperplane and touches the closest data points from each class.\n",
    "\n",
    "For example, in the same binary classification problem as before, the marginal plane can be visualized as two lines that are parallel to the decision boundary and touch the closest data points from each class, \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8b0a1-d631-470a-915d-77e4a5b9df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hard margin:\n",
    "In a hard-margin SVM, the decision boundary is required to separate the two classes of data points with a margin of at least a certain width, regardless of the complexity of the decision boundary. This means that the SVM is only able to find a solution if the data is perfectly separable.\n",
    "\n",
    "For example, consider a binary classification problem where the goal is to classify a set of data points into two classes: blue and red. The data points are two-dimensional, with x and y coordinates. A hard-margin SVM could be used to find a decision boundary that separates the two classes with a margin of at least 1. The decision boundary would be a straight line in this case.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9807c-9d4d-480c-932a-14eff4b9eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Soft margin:\n",
    "In a soft-margin SVM, the decision boundary is allowed to have some misclassifications and a smaller margin in order to handle non-separable data. The soft-margin SVM introduces a slack variable that allows some data points to be misclassified or to be inside the margin.\n",
    "\n",
    "For example, consider a binary classification problem where the goal is to classify a set of data points into two classes: blue and red. The data points are two-dimensional, with x and y coordinates. A soft-margin SVM could be used to find a decision boundary that separates the two classes with a margin of at least 1, allowing for some misclassifications. The decision boundary would be a curved line in this case.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b66eaad-c593-4a77-9af5-6979c6e5f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e22dbb-4f11-4f55-a664-7dd71a215133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78b2f27-ff2e-4167-acce-f2beab03ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svm_clf = LinearSVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy score: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc0ce2b-3381-44d7-8e9e-bfa603910f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy score: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2242bd-05ba-45ff-b960-70e7090c9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the two features to plot\n",
    "X = X_train[:, :2]\n",
    "y = y_train\n",
    "\n",
    "# Create a mesh grid of points to classify\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "# Predict the class of each point in the mesh grid\n",
    "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundaries and the training set\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Linear SVM decision boundaries')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
